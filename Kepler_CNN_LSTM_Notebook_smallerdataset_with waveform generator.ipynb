{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True dataset made\n",
      "False dataset made\n",
      "Test and Train sets made\n",
      "Loading data...\n",
      "4000 train sequences\n",
      "1000 test sequences\n",
      "Build model...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 25600000 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5e9e8e83316a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m model.compile(loss='binary_crossentropy',  # configure the learning process after the model is built well.\n\u001b[1;32m    173\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m               class_mode='binary')\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahul/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, class_mode, sample_weight_mode)\u001b[0m\n\u001b[1;32m    500\u001b[0m         updates = self.optimizer.get_updates(self.trainable_weights,\n\u001b[1;32m    501\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                                              train_loss)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahul/anaconda/lib/python2.7/site-packages/keras/optimizers.pyc\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, params, constraints, loss)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# zero init of velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahul/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m     '''\n\u001b[1;32m     33\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahul/anaconda/lib/python2.7/site-packages/theano/compile/sharedvalue.pyc\u001b[0m in \u001b[0;36mshared\u001b[0;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[0;32m--> 247\u001b[0;31m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahul/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/var.pyc\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[0;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# function requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mdeviceval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ('Error allocating 25600000 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "'''Train a recurrent convolutional network on the IMDB sentiment\n",
    "classification task.\n",
    "GPU command:\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kepler_cnn_lstm.py\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numSamples = 100 # Length of each sample\n",
    "minPeriod = 5\n",
    "maxPeriod = 30\n",
    "noiseAmplitude = 1 #Noise amplitude as a percent of signal amplitude in the pulses\n",
    "\n",
    "numTrue = 2500\n",
    "numFalse =2500\n",
    "X_True = []\n",
    "X_False = []\n",
    "\n",
    "testSetSize = 0.2 #Size of test size as a fraction of total dataset size\n",
    "\n",
    "for i in range(numTrue) :\n",
    "    \n",
    "    \n",
    "    period = random.randint(minPeriod , maxPeriod)  # period\n",
    "    width = random.randint( int(minPeriod/2) , int(maxPeriod/2))   # width of pulse\n",
    "\n",
    "    signal = np.arange(numSamples) % period < width\n",
    "    signal = signal.astype(int)\n",
    "    signal = signal * (-1)\n",
    "    signal = signal * 100\n",
    "\n",
    "    noise = noiseAmplitude * ( np.random.normal(0, 1, numSamples) )\n",
    "\n",
    "    finalSignal = signal + noise \n",
    "    X_True.append(finalSignal.tolist())\n",
    "\n",
    "    #plt.plot(finalSignal)\n",
    "    #plt.ylim(-200, 200)\n",
    "    #plt.xlim(0, 300)\n",
    "    #plt.show()\n",
    "    \n",
    "y_True = np.ones(numTrue).tolist()\n",
    "print(\"True dataset made\")\n",
    "\n",
    "\n",
    "for i in range(numFalse) :\n",
    "    blankNoise = 50 * ( np.random.normal(0, 1, numSamples) )  #Adjust amplitude of blank noise\n",
    "    \n",
    "    X_False.append(blankNoise.tolist())\n",
    "    \n",
    "    #plt.plot(blankNoise)\n",
    "    #plt.ylim(-200, 200)\n",
    "    #plt.xlim(0, 300)\n",
    "    #plt.show()\n",
    "\n",
    "y_False = np.zeros(numFalse).tolist()\n",
    "print(\"False dataset made\")\n",
    "\n",
    "X = X_True + X_False\n",
    "y = y_True + y_False\n",
    "\n",
    "\n",
    "testIdx = random.sample(range(0, len(X)), int(testSetSize * len(X) ))\n",
    "trainIdx = list( set(range(0, len(X))) - set(testIdx) )\n",
    "random.shuffle(trainIdx)\n",
    "X_test = [X[i] for i in testIdx]\n",
    "X_train = [X[i] for i in trainIdx]\n",
    "y_test = [y[i] for i in testIdx]\n",
    "y_train = [y[i] for i in trainIdx]\n",
    "print(\"Test and Train sets made\")\n",
    "\n",
    "\n",
    "\n",
    "# Embedding: Turn positive integers (indexes) into dense vectors of fixed size\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 3 #The extension (spatial or temporal) of each filter\n",
    "nb_filter = 64 #Number of convolution kernels to use (dimensionality of the output)\n",
    "pool_length = 2 # factor by which to downscale. 2 will halve the input.\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 16 # # of samples used to compute the state, input at one time.\n",
    "nb_epoch = 5\n",
    "\n",
    "#print('Loading data...')\n",
    "#data_file1 = \"x-3d4hr_0210_training_nor.csv\"\n",
    "#data_file2 = \"x-3d4hr_0210_testing_nor.csv\"\n",
    "#data_file3 = \"y-3d4hr_0210_training.csv\"\n",
    "#data_file4 = \"y-3d4hr_0210_testing.csv\"\n",
    "\n",
    "# data loading\n",
    "#X_train = pd.read_csv(data_file1, delimiter=',', error_bad_lines=False, header=None)\n",
    "#X_train = X_train.as_matrix()\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "#y_train = pd.read_csv(data_file3, delimiter=',', error_bad_lines=False, header=None)\n",
    "#y_train = y_train.as_matrix()\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#X_test = pd.read_csv(data_file2, delimiter=',', error_bad_lines=False, header=None)\n",
    "#X_test = X_test.as_matrix()\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "#y_test = pd.read_csv(data_file4, delimiter=',', error_bad_lines=False, header=None)\n",
    "#y_test = y_test.as_matrix()\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n",
    "\n",
    "#print(raw_input('123...'))\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "#print('Pad sequences (samples x time)')\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "#X_train = X_train[0:500]\n",
    "#y_train = y_train[0:500]\n",
    "#X_test  = X_test[0:100]\n",
    "#y_test  = y_test[0:100]\n",
    "\n",
    "\n",
    "\n",
    "#print('X_train shape:', X_train.shape)\n",
    "#print('X_test shape:', X_test.shape)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "#print(raw_input('123...'))\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length)) #Max pooling operation for temporal data\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1)) #regular fully connected NN layer, the output dimension is one\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',  # configure the learning process after the model is built well.\n",
    "              optimizer='adam',\n",
    "              class_mode='binary')\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test), show_accuracy=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X_train[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
